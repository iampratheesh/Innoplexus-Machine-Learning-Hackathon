{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Innoplexus_Deep_Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXcGJkt_4gUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0spkAzS4lkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and \n",
        "# statistical natural language processing for English written in the Python programming language.\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "#TQDM is a progress bar library with good support for nested loops and Jupyter/IPython notebooks.\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVXLyCxB5a-P",
        "colab_type": "code",
        "outputId": "96e027f1-f716-491c-aff5-b401e36c46a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "import random\n",
        "from tensorflow import set_random_seed\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Dense,Dropout,Embedding,LSTM\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpslqUtF5cGh",
        "colab_type": "code",
        "outputId": "aeba5c43-35f1-4873-951e-e685340eea5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train= pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>unique_hash</th>\n",
              "      <th>text</th>\n",
              "      <th>drug</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0</td>\n",
              "      <td>Autoimmune diseases tend to come in clusters. ...</td>\n",
              "      <td>gilenya</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9eba8f80e7e20f3a2f48685530748fbfa95943e4</td>\n",
              "      <td>I can completely understand why youâ€™d want to ...</td>\n",
              "      <td>gilenya</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fe809672251f6bd0d986e00380f48d047c7e7b76</td>\n",
              "      <td>Interesting that it only targets S1P-1/5 recep...</td>\n",
              "      <td>fingolimod</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bd22104dfa9ec80db4099523e03fae7a52735eb6</td>\n",
              "      <td>Very interesting, grand merci. Now I wonder wh...</td>\n",
              "      <td>ocrevus</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b227688381f9b25e5b65109dd00f7f895e838249</td>\n",
              "      <td>Hi everybody, My latest MRI results for Brain ...</td>\n",
              "      <td>gilenya</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                unique_hash  ... sentiment\n",
              "0  2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0  ...         2\n",
              "1  9eba8f80e7e20f3a2f48685530748fbfa95943e4  ...         2\n",
              "2  fe809672251f6bd0d986e00380f48d047c7e7b76  ...         2\n",
              "3  bd22104dfa9ec80db4099523e03fae7a52735eb6  ...         2\n",
              "4  b227688381f9b25e5b65109dd00f7f895e838249  ...         1\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SptAGyW5eGU",
        "colab_type": "code",
        "outputId": "a4cfa1d2-0fb9-4d14-a5ae-893bab2b9a2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5279, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmqSTp5k5gX9",
        "colab_type": "code",
        "outputId": "5e1b69d1-391c-4ea2-c722-a2ca15e3df6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>unique_hash</th>\n",
              "      <th>text</th>\n",
              "      <th>drug</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9e9a8166b84114aca147bf409f6f956635034c08</td>\n",
              "      <td>256 (previously stable on natalizumab), with 5...</td>\n",
              "      <td>fingolimod</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>e747e6822c867571afe7b907b51f0f2ca67b0e1a</td>\n",
              "      <td>On fingolimod and have been since December 201...</td>\n",
              "      <td>fingolimod</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50b6d851bcff4f35afe354937949e9948975adf7</td>\n",
              "      <td>Apparently it's shingles! :-/ I do have a few ...</td>\n",
              "      <td>humira</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae</td>\n",
              "      <td>If the Docetaxel doing once a week x3 weeks th...</td>\n",
              "      <td>tagrisso</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8b37d169dee5bdae27060949242fb54feb6a7f7f</td>\n",
              "      <td>CC, Stelara worked in a matter of days for me....</td>\n",
              "      <td>stelara</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                unique_hash  ...        drug\n",
              "0  9e9a8166b84114aca147bf409f6f956635034c08  ...  fingolimod\n",
              "1  e747e6822c867571afe7b907b51f0f2ca67b0e1a  ...  fingolimod\n",
              "2  50b6d851bcff4f35afe354937949e9948975adf7  ...      humira\n",
              "3  7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae  ...    tagrisso\n",
              "4  8b37d169dee5bdae27060949242fb54feb6a7f7f  ...     stelara\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWolfGjl5iLU",
        "colab_type": "code",
        "outputId": "6d8a9a1a-bbc4-4ed6-fe84-6a593c847317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2924, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUuA2ZHQ6J3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_sentences(df):\n",
        "    reviews = []\n",
        "\n",
        "    for sent in tqdm(df['text']):\n",
        "        \n",
        "        #remove html content\n",
        "        review_text = BeautifulSoup(sent).get_text()\n",
        "        \n",
        "        #remove non-alphabetic characters\n",
        "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    \n",
        "        #tokenize the sentences\n",
        "        words = word_tokenize(review_text.lower())\n",
        "    \n",
        "        #lemmatize each word to its lemma\n",
        "        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n",
        "    \n",
        "        reviews.append(lemma_words)\n",
        "\n",
        "    return(reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYJOxh876L3j",
        "colab_type": "code",
        "outputId": "ead01c65-31d2-4f55-a6f7-56e66ab82470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_sentences = clean_sentences(train)\n",
        "test_sentences = clean_sentences(test)\n",
        "print(len(train_sentences))\n",
        "print(len(test_sentences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5279/5279 [00:19<00:00, 276.28it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2924/2924 [00:11<00:00, 265.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "5279\n",
            "2924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUlmHjttczh_",
        "colab_type": "code",
        "outputId": "3082b44d-9521-40ba-fc14-3b07033c597c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_sentences[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['autoimmune',\n",
              "  'disease',\n",
              "  'tend',\n",
              "  'to',\n",
              "  'come',\n",
              "  'in',\n",
              "  'cluster',\n",
              "  'a',\n",
              "  'for',\n",
              "  'gilenya',\n",
              "  'if',\n",
              "  'you',\n",
              "  'feel',\n",
              "  'good',\n",
              "  'don',\n",
              "  't',\n",
              "  'think',\n",
              "  'about',\n",
              "  'it',\n",
              "  'it',\n",
              "  'won',\n",
              "  't',\n",
              "  'change',\n",
              "  'anything',\n",
              "  'but',\n",
              "  'waste',\n",
              "  'your',\n",
              "  'time',\n",
              "  'and',\n",
              "  'energy',\n",
              "  'i',\n",
              "  'm',\n",
              "  'taking',\n",
              "  'tysabri',\n",
              "  'and',\n",
              "  'feel',\n",
              "  'amazing',\n",
              "  'no',\n",
              "  'symptom',\n",
              "  'other',\n",
              "  'than',\n",
              "  'dodgy',\n",
              "  'color',\n",
              "  'vision',\n",
              "  'but',\n",
              "  'i',\n",
              "  've',\n",
              "  'had',\n",
              "  'it',\n",
              "  'since',\n",
              "  'always',\n",
              "  'so',\n",
              "  'don',\n",
              "  't',\n",
              "  'know',\n",
              "  'and',\n",
              "  'i',\n",
              "  'don',\n",
              "  't',\n",
              "  'know',\n",
              "  'if',\n",
              "  'it',\n",
              "  'will',\n",
              "  'last',\n",
              "  'a',\n",
              "  'month',\n",
              "  'a',\n",
              "  'year',\n",
              "  'a',\n",
              "  'decade',\n",
              "  'ive',\n",
              "  'just',\n",
              "  'decided',\n",
              "  'to',\n",
              "  'enjoy',\n",
              "  'the',\n",
              "  'ride',\n",
              "  'no',\n",
              "  'point',\n",
              "  'in',\n",
              "  'worrying'],\n",
              " ['i',\n",
              "  'can',\n",
              "  'completely',\n",
              "  'understand',\n",
              "  'why',\n",
              "  'you',\n",
              "  'd',\n",
              "  'want',\n",
              "  'to',\n",
              "  'try',\n",
              "  'it',\n",
              "  'but',\n",
              "  'result',\n",
              "  'reported',\n",
              "  'in',\n",
              "  'lecture',\n",
              "  'don',\n",
              "  't',\n",
              "  'always',\n",
              "  'stand',\n",
              "  'up',\n",
              "  'to',\n",
              "  'the',\n",
              "  'scrutiny',\n",
              "  'of',\n",
              "  'peer',\n",
              "  'review',\n",
              "  'during',\n",
              "  'publication',\n",
              "  'there',\n",
              "  'so',\n",
              "  'much',\n",
              "  'still',\n",
              "  'to',\n",
              "  'do',\n",
              "  'before',\n",
              "  'this',\n",
              "  'is',\n",
              "  'convincing',\n",
              "  'i',\n",
              "  'hope',\n",
              "  'that',\n",
              "  'it',\n",
              "  'doe',\n",
              "  'work',\n",
              "  'out',\n",
              "  'i',\n",
              "  'really',\n",
              "  'do',\n",
              "  'and',\n",
              "  'if',\n",
              "  'you',\n",
              "  're',\n",
              "  'aware',\n",
              "  'of',\n",
              "  'and',\n",
              "  'happy',\n",
              "  'with',\n",
              "  'the',\n",
              "  'risk',\n",
              "  'then',\n",
              "  'that',\n",
              "  's',\n",
              "  'great',\n",
              "  'i',\n",
              "  'just',\n",
              "  'think',\n",
              "  'it',\n",
              "  's',\n",
              "  'important',\n",
              "  'to',\n",
              "  'present',\n",
              "  'this',\n",
              "  'in',\n",
              "  'a',\n",
              "  'balanced',\n",
              "  'way',\n",
              "  'and',\n",
              "  'to',\n",
              "  'understand',\n",
              "  'why',\n",
              "  'we',\n",
              "  'don',\n",
              "  't',\n",
              "  'move',\n",
              "  'straight',\n",
              "  'from',\n",
              "  'the',\n",
              "  'first',\n",
              "  'show',\n",
              "  'of',\n",
              "  'promise',\n",
              "  'in',\n",
              "  'an',\n",
              "  'animal',\n",
              "  'study',\n",
              "  'to',\n",
              "  'using',\n",
              "  'drug',\n",
              "  'on',\n",
              "  'human',\n",
              "  'there',\n",
              "  's',\n",
              "  'still',\n",
              "  'a',\n",
              "  'lot',\n",
              "  'of',\n",
              "  'animal',\n",
              "  'data',\n",
              "  'to',\n",
              "  'gather',\n",
              "  'and',\n",
              "  'human',\n",
              "  'data',\n",
              "  'to',\n",
              "  'gather',\n",
              "  'before',\n",
              "  'anyone',\n",
              "  'can',\n",
              "  'tell',\n",
              "  'if',\n",
              "  'it',\n",
              "  's',\n",
              "  'safe',\n",
              "  'or',\n",
              "  'effective',\n",
              "  'i',\n",
              "  'can',\n",
              "  't',\n",
              "  'tell',\n",
              "  'you',\n",
              "  'how',\n",
              "  'many',\n",
              "  'time',\n",
              "  'animal',\n",
              "  'study',\n",
              "  'don',\n",
              "  't',\n",
              "  'follow',\n",
              "  'through',\n",
              "  'to',\n",
              "  'human',\n",
              "  'but',\n",
              "  'it',\n",
              "  's',\n",
              "  'one',\n",
              "  'of',\n",
              "  'the',\n",
              "  'major',\n",
              "  'attrition',\n",
              "  'point',\n",
              "  'in',\n",
              "  'drug',\n",
              "  'development',\n",
              "  'you',\n",
              "  've',\n",
              "  'been',\n",
              "  'through',\n",
              "  'some',\n",
              "  'of',\n",
              "  'the',\n",
              "  'unpredictability',\n",
              "  'issue',\n",
              "  'with',\n",
              "  'cladribine',\n",
              "  'gilenya',\n",
              "  'where',\n",
              "  'there',\n",
              "  'wa',\n",
              "  'an',\n",
              "  'interaction',\n",
              "  'that',\n",
              "  'wasn',\n",
              "  't',\n",
              "  'predicted',\n",
              "  'but',\n",
              "  'once',\n",
              "  'people',\n",
              "  'try',\n",
              "  'it',\n",
              "  'the',\n",
              "  'doctor',\n",
              "  'can',\n",
              "  'see',\n",
              "  'pattern',\n",
              "  'and',\n",
              "  'work',\n",
              "  'out',\n",
              "  'what',\n",
              "  's',\n",
              "  'going',\n",
              "  'on',\n",
              "  'clemastine',\n",
              "  'metformin',\n",
              "  'is',\n",
              "  'very',\n",
              "  'exciting',\n",
              "  'and',\n",
              "  'given',\n",
              "  'what',\n",
              "  'you',\n",
              "  've',\n",
              "  'said',\n",
              "  'about',\n",
              "  'your',\n",
              "  'current',\n",
              "  'condition',\n",
              "  'and',\n",
              "  'your',\n",
              "  'personal',\n",
              "  'risk',\n",
              "  'tolerance',\n",
              "  'it',\n",
              "  'make',\n",
              "  'sense',\n",
              "  'to',\n",
              "  'try',\n",
              "  'it',\n",
              "  'it',\n",
              "  'definitely',\n",
              "  'wouldn',\n",
              "  't',\n",
              "  'be',\n",
              "  'for',\n",
              "  'everyone'],\n",
              " ['interesting',\n",
              "  'that',\n",
              "  'it',\n",
              "  'only',\n",
              "  'target',\n",
              "  's',\n",
              "  'p',\n",
              "  'receptor',\n",
              "  'rather',\n",
              "  'than',\n",
              "  'like',\n",
              "  'fingolimod',\n",
              "  'hoping',\n",
              "  'to',\n",
              "  'soon',\n",
              "  'see',\n",
              "  'what',\n",
              "  'the',\n",
              "  'aes',\n",
              "  'and',\n",
              "  'saes',\n",
              "  'were',\n",
              "  'yes',\n",
              "  'i',\n",
              "  'm',\n",
              "  'not',\n",
              "  'sure',\n",
              "  'what',\n",
              "  'this',\n",
              "  'mean',\n",
              "  'exactly',\n",
              "  'quote',\n",
              "  'nine',\n",
              "  'patient',\n",
              "  'reported',\n",
              "  'serious',\n",
              "  'adverse',\n",
              "  'event',\n",
              "  'mg',\n",
              "  'mg',\n",
              "  'mg',\n",
              "  'and',\n",
              "  'mg',\n",
              "  'no',\n",
              "  'serious',\n",
              "  'adverse',\n",
              "  'event',\n",
              "  'wa',\n",
              "  'reported',\n",
              "  'for',\n",
              "  'more',\n",
              "  'than',\n",
              "  'patient',\n",
              "  'and',\n",
              "  'no',\n",
              "  'new',\n",
              "  'safety',\n",
              "  'signal',\n",
              "  'occurred',\n",
              "  'compared',\n",
              "  'with',\n",
              "  'the',\n",
              "  'bold',\n",
              "  'study',\n",
              "  'if',\n",
              "  'there',\n",
              "  'were',\n",
              "  'patient',\n",
              "  'reporting',\n",
              "  'saes',\n",
              "  'how',\n",
              "  'can',\n",
              "  'it',\n",
              "  'be',\n",
              "  'stated',\n",
              "  'that',\n",
              "  'no',\n",
              "  'serious',\n",
              "  'adverse',\n",
              "  'event',\n",
              "  'wa',\n",
              "  'reported',\n",
              "  'for',\n",
              "  'more',\n",
              "  'than',\n",
              "  'patient',\n",
              "  'maybe',\n",
              "  'i',\n",
              "  'haven',\n",
              "  't',\n",
              "  'read',\n",
              "  'this',\n",
              "  'right',\n",
              "  'or',\n",
              "  'maybe',\n",
              "  'there',\n",
              "  's',\n",
              "  'a',\n",
              "  'misprint',\n",
              "  'i',\n",
              "  'm',\n",
              "  'very',\n",
              "  'pleased',\n",
              "  'that',\n",
              "  'something',\n",
              "  'is',\n",
              "  'being',\n",
              "  'developed',\n",
              "  'for',\n",
              "  'spms',\n",
              "  'and',\n",
              "  'it',\n",
              "  's',\n",
              "  'encouraging',\n",
              "  'that',\n",
              "  'siponimod',\n",
              "  'doesn',\n",
              "  't',\n",
              "  'linger',\n",
              "  'for',\n",
              "  'very',\n",
              "  'long',\n",
              "  'in',\n",
              "  'the',\n",
              "  'body'],\n",
              " ['very',\n",
              "  'interesting',\n",
              "  'grand',\n",
              "  'merci',\n",
              "  'now',\n",
              "  'i',\n",
              "  'wonder',\n",
              "  'where',\n",
              "  'lemtrada',\n",
              "  'and',\n",
              "  'ocrevus',\n",
              "  'sale',\n",
              "  'would',\n",
              "  'go',\n",
              "  'if',\n",
              "  'they',\n",
              "  'prove',\n",
              "  'anti',\n",
              "  'cd',\n",
              "  'are',\n",
              "  'induction'],\n",
              " ['hi',\n",
              "  'everybody',\n",
              "  'my',\n",
              "  'latest',\n",
              "  'mri',\n",
              "  'result',\n",
              "  'for',\n",
              "  'brain',\n",
              "  'and',\n",
              "  'cervical',\n",
              "  'cord',\n",
              "  'are',\n",
              "  'in',\n",
              "  'and',\n",
              "  'my',\n",
              "  'next',\n",
              "  'neurologist',\n",
              "  'appointment',\n",
              "  'is',\n",
              "  'in',\n",
              "  'the',\n",
              "  'next',\n",
              "  'couple',\n",
              "  'of',\n",
              "  'week',\n",
              "  'there',\n",
              "  're',\n",
              "  'no',\n",
              "  'new',\n",
              "  'lesion',\n",
              "  'in',\n",
              "  'brain',\n",
              "  'cord',\n",
              "  'and',\n",
              "  'i',\n",
              "  've',\n",
              "  'had',\n",
              "  'no',\n",
              "  'relapse',\n",
              "  'while',\n",
              "  'i',\n",
              "  'wa',\n",
              "  'on',\n",
              "  'gilenya',\n",
              "  'this',\n",
              "  'wa',\n",
              "  'a',\n",
              "  'good',\n",
              "  'sign',\n",
              "  'but',\n",
              "  'there',\n",
              "  'wa',\n",
              "  'one',\n",
              "  'line',\n",
              "  'in',\n",
              "  'the',\n",
              "  'cervical',\n",
              "  'cord',\n",
              "  'review',\n",
              "  'that',\n",
              "  'concerned',\n",
              "  'me',\n",
              "  'it',\n",
              "  'go',\n",
              "  'lesion',\n",
              "  'at',\n",
              "  'c',\n",
              "  'and',\n",
              "  't',\n",
              "  'now',\n",
              "  'show',\n",
              "  'hypointensity',\n",
              "  'on',\n",
              "  'the',\n",
              "  'post',\n",
              "  'gadolinium',\n",
              "  't',\n",
              "  'image',\n",
              "  'only',\n",
              "  'this',\n",
              "  'could',\n",
              "  'represent',\n",
              "  'artifact',\n",
              "  'or',\n",
              "  'early',\n",
              "  'axonal',\n",
              "  'loss',\n",
              "  'that',\n",
              "  'wa',\n",
              "  'bothersome',\n",
              "  'to',\n",
              "  'read',\n",
              "  'what',\n",
              "  'are',\n",
              "  'the',\n",
              "  'kind',\n",
              "  'of',\n",
              "  'symptom',\n",
              "  'from',\n",
              "  'c',\n",
              "  'c',\n",
              "  'lesion',\n",
              "  'should',\n",
              "  'i',\n",
              "  'be',\n",
              "  'aware',\n",
              "  'of',\n",
              "  'would',\n",
              "  'it',\n",
              "  'result',\n",
              "  'in',\n",
              "  'change',\n",
              "  'of',\n",
              "  'my',\n",
              "  'dmt',\n",
              "  'thanks']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_How66BjEKHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJnR3gQnEQ9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNLvAyFn6Xhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target=train.sentiment.values\n",
        "y_target=to_categorical(target)\n",
        "num_classes=y_target.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2o2LaTDah8G",
        "colab_type": "code",
        "outputId": "cd69d9e7-f9ad-4ba3-ced9-8923ebe63ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_target.shape[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHzK6oL-6naL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_val,y_train,y_val=train_test_split(train_sentences,y_target,test_size=0.2,stratify=y_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7UakkVCMUp-",
        "colab_type": "code",
        "outputId": "5c279293-26f6-4484-b35a-1c7fa205b264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(X_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4083"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTQawWq5BxNX",
        "colab_type": "code",
        "outputId": "bb8e9e0d-13b5-4095-8383-b3750c741d74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKYdNLzb6pFD",
        "colab_type": "code",
        "outputId": "817cbbda-cd61-49a6-886c-d7331a569a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#It is needed for initializing tokenizer of keras and subsequent padding\n",
        "\n",
        "unique_words = set()\n",
        "len_max = 0\n",
        "\n",
        "for i,sent in tqdm(enumerate(X_train)):\n",
        "    \n",
        "    unique_words.update(sent)\n",
        "    \n",
        "    if(len_max<len(sent) and len(sent)<=1713):\n",
        "        len_max = len(sent)\n",
        "        print(len_max)\n",
        "    elif len(sent)>1713:\n",
        "      del X_train[i]\n",
        "      y_train = np.delete(y_train, i, axis = 0)\n",
        "        \n",
        "#length of the list of unique_words gives the no of unique words\n",
        "print(len(list(unique_words)))\n",
        "print(len_max)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4083it [00:00, 40760.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "87\n",
            "1061\n",
            "1280\n",
            "1385\n",
            "1475\n",
            "1491\n",
            "1571\n",
            "1583\n",
            "1649\n",
            "1698\n",
            "1713\n",
            "30378\n",
            "1713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeOrgsTiBFpy",
        "colab_type": "code",
        "outputId": "75e6a526-ef08-4e90-92a1-8d3d4f30bfa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meAlwP876q3g",
        "colab_type": "code",
        "outputId": "d0d632c9-764b-432f-c6cb-8f0571f307a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
        "tokenizer.fit_on_texts(list(X_train))\n",
        "\n",
        "#texts_to_sequences(texts)\n",
        "\n",
        "    # Arguments- texts: list of texts to turn to sequences.\n",
        "    #Return: list of sequences (one per text input).\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "X_test = tokenizer.texts_to_sequences(test_sentences)\n",
        "\n",
        "#padding done to equalize the lengths of all input reviews. LSTM networks needs all inputs to be same length.\n",
        "#Therefore reviews lesser than max length will be made equal using extra zeros at end. This is padding.\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=len_max)\n",
        "X_val = sequence.pad_sequences(X_val, maxlen=len_max)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=len_max)\n",
        "\n",
        "print(X_train.shape,X_val.shape,X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4083, 1713) (1056, 1713) (2924, 1713)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwCZv3RT6tub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor='val_acc', patience = 2)\n",
        "callback = [early_stopping]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHyk68M06wMO",
        "colab_type": "code",
        "outputId": "ebc9898d-dc12-4e9b-8c76-88bc45856aed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "#Model using Keras LSTM\n",
        "\n",
        "#Multilayer Perceptron (MLP) for multi-class softmax classification:\n",
        "#Letâ€™s build whatâ€™s probably the most popular type of model in NLP at the moment: Long Short Term Memory network. \n",
        "#This architecture is specially designed to work on sequence data.\n",
        "#It fits perfectly for many NLP tasks like tagging and text classification.\n",
        "#It treats the text as a sequence rather than a bag of words or as ngrams.\n",
        "\n",
        "#Hereâ€™s a possible model definition:\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(len(list(unique_words)),300,input_length=len_max))\n",
        "model.add(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=True))\n",
        "model.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
        "model.add(Dense(100,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(50,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes,activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.005),metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 1713, 300)         9113400   \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 1713, 128)         219648    \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               6500      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 9,394,159\n",
            "Trainable params: 9,394,159\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUyPyjng623P",
        "colab_type": "code",
        "outputId": "175f8517-c981-4939-82c5-ca7abc3b4250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "#This is done for learning purpose only. One can play around with different hyper parameters combinations\n",
        "#and try increase the accuracy even more. For example, a different learning rate, an extra dense layer \n",
        "# before output layer, etc. Cross validation could be used to evaluate the model and grid search \n",
        "# further to find unique combination of parameters that give maximum accuracy. This model has a validation\n",
        "#accuracy of around 66.5%\n",
        "history=model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=15, batch_size=256, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4083 samples, validate on 1056 samples\n",
            "Epoch 1/15\n",
            "4083/4083 [==============================] - 114s 28ms/step - loss: 0.8630 - acc: 0.6936 - val_loss: 0.7597 - val_acc: 0.7244\n",
            "Epoch 2/15\n",
            "4083/4083 [==============================] - 111s 27ms/step - loss: 0.7347 - acc: 0.7213 - val_loss: 0.7634 - val_acc: 0.7244\n",
            "Epoch 3/15\n",
            "4083/4083 [==============================] - 110s 27ms/step - loss: 0.6377 - acc: 0.7291 - val_loss: 0.8539 - val_acc: 0.6487\n",
            "Epoch 4/15\n",
            "4083/4083 [==============================] - 110s 27ms/step - loss: 0.5460 - acc: 0.7624 - val_loss: 0.9383 - val_acc: 0.6420\n",
            "Epoch 5/15\n",
            "4083/4083 [==============================] - 109s 27ms/step - loss: 0.4824 - acc: 0.7901 - val_loss: 0.9905 - val_acc: 0.6723\n",
            "Epoch 6/15\n",
            "4083/4083 [==============================] - 109s 27ms/step - loss: 0.4277 - acc: 0.8129 - val_loss: 1.1196 - val_acc: 0.6572\n",
            "Epoch 7/15\n",
            "4083/4083 [==============================] - 109s 27ms/step - loss: 0.3816 - acc: 0.8261 - val_loss: 1.3854 - val_acc: 0.6345\n",
            "Epoch 8/15\n",
            "4083/4083 [==============================] - 109s 27ms/step - loss: 0.3365 - acc: 0.8415 - val_loss: 1.6950 - val_acc: 0.6629\n",
            "Epoch 9/15\n",
            "4083/4083 [==============================] - 109s 27ms/step - loss: 0.3272 - acc: 0.8479 - val_loss: 1.5391 - val_acc: 0.6629\n",
            "Epoch 10/15\n",
            "4083/4083 [==============================] - 108s 27ms/step - loss: 0.3270 - acc: 0.8528 - val_loss: 1.6328 - val_acc: 0.6506\n",
            "Epoch 11/15\n",
            "4083/4083 [==============================] - 109s 27ms/step - loss: 0.2951 - acc: 0.8577 - val_loss: 2.2082 - val_acc: 0.6193\n",
            "Epoch 12/15\n",
            "4083/4083 [==============================] - 108s 27ms/step - loss: 0.2863 - acc: 0.8582 - val_loss: 1.7834 - val_acc: 0.6326\n",
            "Epoch 13/15\n",
            "4083/4083 [==============================] - 108s 27ms/step - loss: 0.2738 - acc: 0.8604 - val_loss: 2.4477 - val_acc: 0.6089\n",
            "Epoch 14/15\n",
            "4083/4083 [==============================] - 108s 27ms/step - loss: 0.2645 - acc: 0.8614 - val_loss: 2.2630 - val_acc: 0.6165\n",
            "Epoch 15/15\n",
            "4083/4083 [==============================] - 108s 27ms/step - loss: 0.2698 - acc: 0.8641 - val_loss: 2.4510 - val_acc: 0.6506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzjnpCRP65uZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred=model.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2wlHV84JjIz",
        "colab_type": "code",
        "outputId": "78b5773e-9ece-4627-930a-04d3ca85cd4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpVfLs6_IjXo",
        "colab_type": "code",
        "outputId": "6576ae8a-4fd0-4bae-f6bf-166dc91924f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "sub_file = pd.read_csv('sample_submission.csv')\n",
        "sub_file.sentiment=y_pred\n",
        "sub_file.to_csv('Deep_Submission.csv',index=False)\n",
        "sub_file.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>unique_hash</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9e9a8166b84114aca147bf409f6f956635034c08</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>e747e6822c867571afe7b907b51f0f2ca67b0e1a</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50b6d851bcff4f35afe354937949e9948975adf7</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8b37d169dee5bdae27060949242fb54feb6a7f7f</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                unique_hash  sentiment\n",
              "0  9e9a8166b84114aca147bf409f6f956635034c08          2\n",
              "1  e747e6822c867571afe7b907b51f0f2ca67b0e1a          1\n",
              "2  50b6d851bcff4f35afe354937949e9948975adf7          2\n",
              "3  7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae          2\n",
              "4  8b37d169dee5bdae27060949242fb54feb6a7f7f          2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYmX-Nv2LaDp",
        "colab_type": "text"
      },
      "source": [
        "## GRID SEARCH CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnQ85auncGv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtW40rDEcN0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(epochs=2, batch_size=256,optimizer='SGD',learn_rate=0.001,activation='softmax'):\n",
        "  model=Sequential()\n",
        "  model.add(Embedding(len(list(unique_words)),50,input_length=len_max))\n",
        "  model.add(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=True))\n",
        "  model.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
        "  model.add(Dense(100,activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes,activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',optimizer=optimizer, metrics=['accuracy'])\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Jgm69icYiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD6p_fWnWNLw",
        "colab_type": "code",
        "outputId": "c83b57b1-6be2-4420-adb6-bf7ac830966d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
        "epochs = [6]\n",
        "batch_size = [256]\n",
        "param_grid = dict(epochs = epochs, batch_size = batch_size, optimizer = optimizer, learn_rate = learn_rate, activation = activation)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring = 'accuracy')\n",
        "grid_result = grid.fit(X_train[:1000], y_train[:1000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "666/666 [==============================] - 79s 118ms/step - loss: 1.0959 - acc: 0.5090\n",
            "Epoch 2/6\n",
            "666/666 [==============================] - 77s 115ms/step - loss: 1.0803 - acc: 0.7387\n",
            "Epoch 3/6\n",
            "666/666 [==============================] - 77s 115ms/step - loss: 1.0642 - acc: 0.7402\n",
            "Epoch 4/6\n",
            "666/666 [==============================] - 77s 115ms/step - loss: 1.0482 - acc: 0.7402\n",
            "Epoch 5/6\n",
            "666/666 [==============================] - 77s 116ms/step - loss: 1.0331 - acc: 0.7402\n",
            "Epoch 6/6\n",
            "666/666 [==============================] - 77s 115ms/step - loss: 1.0164 - acc: 0.7402\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 79s 119ms/step - loss: 1.0941 - acc: 0.5652\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 77s 116ms/step - loss: 1.0782 - acc: 0.7301\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 77s 116ms/step - loss: 1.0610 - acc: 0.7301\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 77s 115ms/step - loss: 1.0442 - acc: 0.7301\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 77s 115ms/step - loss: 1.0272 - acc: 0.7301\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 77s 115ms/step - loss: 1.0118 - acc: 0.7301\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 80s 120ms/step - loss: 1.0946 - acc: 0.5187\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 77s 115ms/step - loss: 1.0813 - acc: 0.7301\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 77s 115ms/step - loss: 1.0661 - acc: 0.7316\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 77s 116ms/step - loss: 1.0496 - acc: 0.7316\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 77s 116ms/step - loss: 1.0352 - acc: 0.7316\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 77s 115ms/step - loss: 1.0210 - acc: 0.7316\n",
            "Epoch 1/6\n",
            "666/666 [==============================] - 80s 120ms/step - loss: 1.0794 - acc: 0.5691\n",
            "Epoch 2/6\n",
            "666/666 [==============================] - 77s 115ms/step - loss: 0.9322 - acc: 0.7402\n",
            "Epoch 3/6\n",
            "666/666 [==============================] - 76s 115ms/step - loss: 0.7753 - acc: 0.7402\n",
            "Epoch 4/6\n",
            "666/666 [==============================] - 76s 114ms/step - loss: 0.7752 - acc: 0.7402\n",
            "Epoch 5/6\n",
            "666/666 [==============================] - 77s 115ms/step - loss: 0.7775 - acc: 0.7402\n",
            "Epoch 6/6\n",
            "666/666 [==============================] - 77s 116ms/step - loss: 0.7583 - acc: 0.7402\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 82s 123ms/step - loss: 1.0761 - acc: 0.5682\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 77s 116ms/step - loss: 0.9435 - acc: 0.7301\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 78s 116ms/step - loss: 0.8046 - acc: 0.7301\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 78s 116ms/step - loss: 0.8043 - acc: 0.7301\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 78s 116ms/step - loss: 0.8023 - acc: 0.7301\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 77s 116ms/step - loss: 0.7854 - acc: 0.7301\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 81s 121ms/step - loss: 1.0678 - acc: 0.6027\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 77s 115ms/step - loss: 0.8905 - acc: 0.7316\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 77s 115ms/step - loss: 0.8064 - acc: 0.7316\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 77s 115ms/step - loss: 0.7988 - acc: 0.7316\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 77s 116ms/step - loss: 0.8002 - acc: 0.7316\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 77s 115ms/step - loss: 0.7720 - acc: 0.7316\n",
            "Epoch 1/6\n",
            "666/666 [==============================] - 81s 122ms/step - loss: 1.0872 - acc: 0.5420\n",
            "Epoch 2/6\n",
            "666/666 [==============================] - 76s 115ms/step - loss: 0.9803 - acc: 0.7312\n",
            "Epoch 3/6\n",
            "666/666 [==============================] - 76s 115ms/step - loss: 0.7633 - acc: 0.7402\n",
            "Epoch 4/6\n",
            "666/666 [==============================] - 77s 115ms/step - loss: 0.7378 - acc: 0.7402\n",
            "Epoch 5/6\n",
            "666/666 [==============================] - 76s 115ms/step - loss: 0.7054 - acc: 0.7402\n",
            "Epoch 6/6\n",
            "666/666 [==============================] - 76s 115ms/step - loss: 0.7237 - acc: 0.7553\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 81s 122ms/step - loss: 1.1477 - acc: 0.5907\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 0.8776 - acc: 0.7301\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 0.8169 - acc: 0.7301\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 0.7654 - acc: 0.7301\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 75s 113ms/step - loss: 0.7405 - acc: 0.7301\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 75s 112ms/step - loss: 0.6573 - acc: 0.7376\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 80s 120ms/step - loss: 1.1648 - acc: 0.5667\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 75s 112ms/step - loss: 0.8787 - acc: 0.7316\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 75s 112ms/step - loss: 0.8339 - acc: 0.7316\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 75s 113ms/step - loss: 0.7788 - acc: 0.7316\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 74s 111ms/step - loss: 0.7404 - acc: 0.7316\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 74s 111ms/step - loss: 0.6594 - acc: 0.7586\n",
            "Epoch 1/6\n",
            "666/666 [==============================] - 82s 123ms/step - loss: 1.0896 - acc: 0.5616\n",
            "Epoch 2/6\n",
            "666/666 [==============================] - 76s 114ms/step - loss: 1.0450 - acc: 0.7402\n",
            "Epoch 3/6\n",
            "666/666 [==============================] - 76s 114ms/step - loss: 0.9584 - acc: 0.7402\n",
            "Epoch 4/6\n",
            "666/666 [==============================] - 75s 113ms/step - loss: 0.8334 - acc: 0.7402\n",
            "Epoch 5/6\n",
            "666/666 [==============================] - 77s 115ms/step - loss: 0.7807 - acc: 0.7402\n",
            "Epoch 6/6\n",
            "666/666 [==============================] - 75s 113ms/step - loss: 0.7884 - acc: 0.7402\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 83s 125ms/step - loss: 1.0919 - acc: 0.5292\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 1.0514 - acc: 0.7301\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 76s 113ms/step - loss: 0.9732 - acc: 0.7301\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 75s 113ms/step - loss: 0.8272 - acc: 0.7301\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 75s 113ms/step - loss: 0.7911 - acc: 0.7301\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 75s 113ms/step - loss: 0.7849 - acc: 0.7301\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 83s 125ms/step - loss: 1.0944 - acc: 0.4918\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 75s 113ms/step - loss: 1.0632 - acc: 0.7316\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 76s 113ms/step - loss: 1.0160 - acc: 0.7316\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 75s 113ms/step - loss: 0.9146 - acc: 0.7316\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 76s 113ms/step - loss: 0.7955 - acc: 0.7316\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 75s 112ms/step - loss: 0.7949 - acc: 0.7316\n",
            "Epoch 1/6\n",
            "666/666 [==============================] - 83s 124ms/step - loss: 1.0888 - acc: 0.6051\n",
            "Epoch 2/6\n",
            "666/666 [==============================] - 76s 113ms/step - loss: 1.0417 - acc: 0.7402\n",
            "Epoch 3/6\n",
            "666/666 [==============================] - 76s 114ms/step - loss: 0.9305 - acc: 0.7402\n",
            "Epoch 4/6\n",
            "666/666 [==============================] - 75s 112ms/step - loss: 0.8186 - acc: 0.7402\n",
            "Epoch 5/6\n",
            "666/666 [==============================] - 74s 111ms/step - loss: 0.8052 - acc: 0.7402\n",
            "Epoch 6/6\n",
            "666/666 [==============================] - 75s 113ms/step - loss: 0.7783 - acc: 0.7402\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 82s 124ms/step - loss: 1.0930 - acc: 0.5142\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 75s 112ms/step - loss: 1.0578 - acc: 0.7301\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 75s 112ms/step - loss: 0.9793 - acc: 0.7301\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 74s 111ms/step - loss: 0.8184 - acc: 0.7301\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 74s 111ms/step - loss: 0.8728 - acc: 0.7301\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 74s 111ms/step - loss: 0.8376 - acc: 0.7301\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 85s 127ms/step - loss: 1.0870 - acc: 0.6537\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 75s 113ms/step - loss: 1.0444 - acc: 0.7316\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 0.9388 - acc: 0.7316\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 74s 112ms/step - loss: 0.8266 - acc: 0.7316\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 74s 111ms/step - loss: 0.8189 - acc: 0.7316\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 74s 111ms/step - loss: 0.7852 - acc: 0.7316\n",
            "Epoch 1/6\n",
            "666/666 [==============================] - 84s 127ms/step - loss: 1.0857 - acc: 0.5811\n",
            "Epoch 2/6\n",
            "666/666 [==============================] - 75s 112ms/step - loss: 1.0151 - acc: 0.7387\n",
            "Epoch 3/6\n",
            "666/666 [==============================] - 75s 112ms/step - loss: 0.8473 - acc: 0.7402\n",
            "Epoch 4/6\n",
            "666/666 [==============================] - 75s 113ms/step - loss: 0.8268 - acc: 0.7402\n",
            "Epoch 5/6\n",
            "666/666 [==============================] - 75s 113ms/step - loss: 0.7956 - acc: 0.7402\n",
            "Epoch 6/6\n",
            "666/666 [==============================] - 75s 112ms/step - loss: 0.7931 - acc: 0.7402\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 86s 129ms/step - loss: 1.0846 - acc: 0.5577\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 1.0061 - acc: 0.7301\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 0.8452 - acc: 0.7301\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 75s 113ms/step - loss: 0.8512 - acc: 0.7301\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 0.7730 - acc: 0.7301\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 0.7858 - acc: 0.7301\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 85s 127ms/step - loss: 1.0839 - acc: 0.5622\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 75s 113ms/step - loss: 1.0090 - acc: 0.7316\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 0.8724 - acc: 0.7316\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 75s 113ms/step - loss: 0.8446 - acc: 0.7316\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 0.8232 - acc: 0.7316\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 0.7743 - acc: 0.7316\n",
            "Epoch 1/6\n",
            "666/666 [==============================] - 85s 127ms/step - loss: 1.0776 - acc: 0.6622\n",
            "Epoch 2/6\n",
            "666/666 [==============================] - 75s 112ms/step - loss: 0.8828 - acc: 0.7402\n",
            "Epoch 3/6\n",
            "666/666 [==============================] - 74s 112ms/step - loss: 0.8043 - acc: 0.7402\n",
            "Epoch 4/6\n",
            "666/666 [==============================] - 75s 112ms/step - loss: 0.7854 - acc: 0.7402\n",
            "Epoch 5/6\n",
            "666/666 [==============================] - 75s 112ms/step - loss: 0.7644 - acc: 0.7402\n",
            "Epoch 6/6\n",
            "666/666 [==============================] - 75s 113ms/step - loss: 0.7166 - acc: 0.7402\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 87s 130ms/step - loss: 1.0845 - acc: 0.5877\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 74s 112ms/step - loss: 0.9413 - acc: 0.7301\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 74s 111ms/step - loss: 0.8547 - acc: 0.7286\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 74s 112ms/step - loss: 0.8221 - acc: 0.7301\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 74s 112ms/step - loss: 0.7692 - acc: 0.7301\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 74s 112ms/step - loss: 0.7674 - acc: 0.7301\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 88s 131ms/step - loss: 1.0815 - acc: 0.6477\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 75s 112ms/step - loss: 0.9185 - acc: 0.7316\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 75s 112ms/step - loss: 0.8090 - acc: 0.7316\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 74s 112ms/step - loss: 0.7894 - acc: 0.7316\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 75s 112ms/step - loss: 0.7849 - acc: 0.7316\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 75s 112ms/step - loss: 0.7762 - acc: 0.7316\n",
            "Epoch 1/6\n",
            "666/666 [==============================] - 88s 131ms/step - loss: 1.0926 - acc: 0.5751\n",
            "Epoch 2/6\n",
            "666/666 [==============================] - 75s 112ms/step - loss: 1.0735 - acc: 0.7402\n",
            "Epoch 3/6\n",
            "666/666 [==============================] - 75s 113ms/step - loss: 1.0524 - acc: 0.7402\n",
            "Epoch 4/6\n",
            "666/666 [==============================] - 75s 113ms/step - loss: 1.0330 - acc: 0.7402\n",
            "Epoch 5/6\n",
            "666/666 [==============================] - 75s 112ms/step - loss: 1.0137 - acc: 0.7402\n",
            "Epoch 6/6\n",
            "666/666 [==============================] - 75s 113ms/step - loss: 0.9935 - acc: 0.7402\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 90s 135ms/step - loss: 1.0946 - acc: 0.5427\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 1.0794 - acc: 0.7286\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 1.0624 - acc: 0.7301\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 76s 113ms/step - loss: 1.0450 - acc: 0.7301\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 1.0285 - acc: 0.7301\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 76s 114ms/step - loss: 1.0118 - acc: 0.7301\n",
            "Epoch 1/6\n",
            "667/667 [==============================] - 88s 132ms/step - loss: 1.0958 - acc: 0.4393\n",
            "Epoch 2/6\n",
            "667/667 [==============================] - 76s 113ms/step - loss: 1.0808 - acc: 0.7271\n",
            "Epoch 3/6\n",
            "667/667 [==============================] - 76s 113ms/step - loss: 1.0636 - acc: 0.7316\n",
            "Epoch 4/6\n",
            "667/667 [==============================] - 75s 113ms/step - loss: 1.0475 - acc: 0.7316\n",
            "Epoch 5/6\n",
            "667/667 [==============================] - 75s 112ms/step - loss: 1.0307 - acc: 0.7316\n",
            "Epoch 6/6\n",
            "667/667 [==============================] - 75s 113ms/step - loss: 1.0154 - acc: 0.7316\n",
            "Epoch 1/6\n",
            "256/666 [==========>...................] - ETA: 1:04 - loss: 1.0990 - acc: 0.3477"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLlZZ6IZ4gEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}